<configuration>
	<property>
		<name>javax.jdo.option.ConnectionURL</name>
		<value>jdbc:mysql://mysql:3306/hive?createDatabaseIfNotExist=true&amp;useUnicode=true&amp;characterEncoding=UTF-8&amp;useSSL=false</value>
		<description>JDBC connect string for a JDBC metastore</description>
	</property>
	<property>
		<name>javax.jdo.option.ConnectionDriverName</name>
		<value>com.mysql.jdbc.Driver</value>
		<description>Driver class name for a JDBC metastore</description>
	</property>
	<property>
		<name>javax.jdo.option.ConnectionUserName</name>
		<value>root</value>
		<description>username to use against metastore database</description>
	</property>
	<property>
		<name>javax.jdo.option.ConnectionPassword</name>
		<value>whg19961119</value>
		<description>password to use against metastore database</description>
	</property>
	<property>
		<name>hive.metastore.uris</name>
		<value>thrift://hive-master:9083</value>
	</property>
	<property>
		<name>hive.metastore.schema.verification</name>
		<value>false</value>
	</property>
	<!--spark engine -->
	<property>
		<name>hive.execution.engine</name>
		<value>spark</value>
	</property>
	<property>
		<name>hive.enable.spark.execution.engine</name>
		<value>true</value>
	</property>
	<!--sparkcontext -->
	<property>
		<name>spark.master</name>
		<value>yarn-cluster</value>
	</property>
	<property>
		<name>spark.serializer</name>
		<value>org.apache.spark.serializer.KryoSerializer</value>
	</property>
	<property>
   		<name>spark.yarn.jars</name>
   		<value>hdfs://hive-master:9000/spark-jars/*</value>
	</property>
	<!--下面的根据实际情况配置 -->
	<property>
		<name>spark.executor.instances</name>
		<value>5</value>
	</property>
	<property>
		<name>spark.executor.cores</name>
		<value>5</value>
	</property>
	<property>
		<name>spark.executor.memory</name>
		<value>1024m</value>
	</property>
	<property>
		<name>spark.driver.cores</name>
		<value>1</value>
	</property>
	<property>
		<name>spark.driver.memory</name>
		<value>1024m</value>
	</property>
	<property>
		<name>spark.yarn.queue</name>
		<value>default</value>
	</property>
	<property>
		<name>spark.app.name</name>
		<value>myInceptor</value>
	</property>

	<!--事务相关 -->
	<property>
		<name>hive.support.concurrency</name>
		<value>true</value>
	</property>
	<property>
		<name>hive.enforce.bucketing</name>
		<value>true</value>
	</property>
	<property>
		<name>hive.exec.dynamic.partition.mode</name>
		<value>nonstrict</value>
	</property>
	<property>
		<name>hive.txn.manager</name>
		<value>org.apache.hadoop.hive.ql.lockmgr.DbTxnManager</value>
	</property>
	<property>
		<name>hive.compactor.initiator.on</name>
		<value>true</value>
	</property>
	<property>
		<name>hive.compactor.worker.threads</name>
		<value>1</value>
	</property>
	<property>
		<name>spark.executor.extraJavaOptions</name>
		<value>-XX:+PrintGCDetails -Dkey=value -Dnumbers="one two three"
		</value>
	</property>
	<!--其它 -->
	<property>
		<name>hive.server2.enable.doAs</name>
		<value>false</value>
	</property>
</configuration>
